{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Добро пожаловать в Colaboratory!",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYArVefvro08"
      },
      "source": [
        "#Задача:\r\n",
        "запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета, сделать выводы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU4--wrAXwae",
        "outputId": "98b5c373-859d-434c-ef46-a95d1f1f25dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.datasets import fetch_20newsgroups\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\r\n",
        "\r\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\r\n",
        "\r\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS,\r\n",
        "                            analyzer='word',min_df = 0.002)\r\n",
        "\r\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\r\n",
        "u = X_train.toarray()\r\n",
        "\r\n",
        "w = []\r\n",
        "for e in range(len(u)):\r\n",
        "    k = []\r\n",
        "    t = u[e].nonzero()[0]\r\n",
        "    for i in range(len(t)):\r\n",
        "        for j in range(u[e][t[i]]):\r\n",
        "            k.append(t[i])\r\n",
        "    w.append(k)\r\n",
        "\r\n",
        "q = np.array(w)\r\n",
        "\r\n",
        "def FormInitModel(arr, words_by_key, z, chosen_words, q):\r\n",
        "    for d, text in enumerate(q):\r\n",
        "        lst = []\r\n",
        "\r\n",
        "        for w in text:\r\n",
        "            p = np.divide(np.multiply(arr[d, :], words_by_key[:, w]), z)\r\n",
        "            a = np.random.multinomial(1, p / p.sum()).argmax()\r\n",
        "            lst.append(a)\r\n",
        "\r\n",
        "            arr[d, a] += 1\r\n",
        "            words_by_key[a, w] += 1\r\n",
        "            z[a] += 1\r\n",
        "\r\n",
        "        chosen_words.append(lst)\r\n",
        "\r\n",
        "\r\n",
        "def sampling(arr,words_by_key,z,chosen_words,q):\r\n",
        "    for d, text in enumerate(q):\r\n",
        "        for i, w in enumerate(text):\r\n",
        "            a = chosen_words[d][i]\r\n",
        "            arr[d, a] -= 1\r\n",
        "            words_by_key[a, w] -= 1\r\n",
        "            z[a] -= 1\r\n",
        "            p = np.divide(np.multiply(arr[d, :], words_by_key[:, w]), z)\r\n",
        "            a = np.random.multinomial(1, p / p.sum()).argmax()\r\n",
        "            chosen_words[d][i] = a\r\n",
        "            arr[d, a] += 1\r\n",
        "            words_by_key[a, w] += 1\r\n",
        "            z[a] += 1\r\n",
        "\r\n",
        "def GetKey(dictionary, value):\r\n",
        "    Keys = list()\r\n",
        "    ItemList = dictionary.items()\r\n",
        "\r\n",
        "    for Item in ItemList:\r\n",
        "        if Item[1] == value:\r\n",
        "            Keys.append(Item[0])\r\n",
        "\r\n",
        "    return Keys\r\n",
        "\r\n",
        "num = len(vectorizer.vocabulary_)\r\n",
        "alpha_param = 5\r\n",
        "beta_param = 0.1\r\n",
        "\r\n",
        "chosen_words = []\r\n",
        "arr = np.zeros([len(q), 20]) + alpha_param\r\n",
        "words_by_key = np.zeros([20, num]) + beta_param\r\n",
        "z = np.zeros([20]) + num * beta_param\r\n",
        "\r\n",
        "FormInitModel(arr,words_by_key,z,chosen_words,q)\r\n",
        "\r\n",
        "for i in range(50):\r\n",
        "  sampling(arr,words_by_key,z,chosen_words,q)\r\n",
        "\r\n",
        "#набираем топ слов\r\n",
        "words_by_topic = []\r\n",
        "\r\n",
        "for topic in range(20):\r\n",
        "  index = words_by_key[topic, :].argsort()[:10]\r\n",
        "  words = []\r\n",
        " \r\n",
        "  for i in index:\r\n",
        "    words.append(GetKey(vectorizer.vocabulary_, i))\r\n",
        "  words_by_topic.append(list(reversed(words)))\r\n",
        "\r\n",
        "for i in range (20):\r\n",
        "    print(i+1, words_by_topic[i],'\\n')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 [['87'], ['rights'], ['archive'], ['email'], ['arabs'], ['los'], ['title'], ['problem'], ['alt'], ['expansion']] \n",
            "\n",
            "2 [['friend'], ['run'], ['manager'], ['cars'], ['versions'], ['completely'], ['postscript'], ['84'], ['bell'], ['won']] \n",
            "\n",
            "3 [['identity'], ['rights'], ['dept'], ['attacks'], ['sounds'], ['period'], ['regular'], ['results'], ['posts'], ['english']] \n",
            "\n",
            "4 [['testament'], ['clinton'], ['greatly'], ['nsa'], ['tests'], ['carry'], ['room'], ['int'], ['client'], ['resurrection']] \n",
            "\n",
            "5 [['crypto'], ['signal'], ['hardware'], ['apply'], ['90'], ['sales'], ['operation'], ['typical'], ['checked'], ['method']] \n",
            "\n",
            "6 [['homosexuals'], ['charge'], ['operating'], ['actually'], ['adam'], ['normally'], ['record'], ['check'], ['university'], ['house']] \n",
            "\n",
            "7 [['consider'], ['internet'], ['processing'], ['buying'], ['apps'], ['looks'], ['report'], ['interface'], ['functions'], ['colormap']] \n",
            "\n",
            "8 [['background'], ['event'], ['meaning'], ['tried'], ['et'], ['postscript'], ['trouble'], ['truly'], ['possibly'], ['death']] \n",
            "\n",
            "9 [['beat'], ['considering'], ['carry'], ['cases'], ['definition'], ['test'], ['quick'], ['300'], ['pc'], ['man']] \n",
            "\n",
            "10 [['hole'], ['traffic'], ['requirements'], ['23'], ['site'], ['sell'], ['requires'], ['com'], ['criminals'], ['display']] \n",
            "\n",
            "11 [['approach'], ['firearms'], ['fired'], ['flame'], ['thomas'], ['follow'], ['national'], ['fpu'], ['funding'], ['john']] \n",
            "\n",
            "12 [['cost'], ['decision'], ['washington'], ['christian'], ['presented'], ['appropriate'], ['putting'], ['screen'], ['shots'], ['goal']] \n",
            "\n",
            "13 [['charge'], ['trying'], ['600'], ['guide'], ['personal'], ['matthew'], ['private'], ['turks'], ['suppose'], ['say']] \n",
            "\n",
            "14 [['different'], ['hospital'], ['certainly'], ['sin'], ['access'], ['simple'], ['use'], ['received'], ['chastity'], ['fbi']] \n",
            "\n",
            "15 [['robert'], ['armenia'], ['effect'], ['mother'], ['government'], ['aid'], ['states'], ['rkba'], ['state'], ['involved']] \n",
            "\n",
            "16 [['sc'], ['id'], ['contrib'], ['ibm'], ['hz'], ['order'], ['universe'], ['rd'], ['upgrade'], ['au']] \n",
            "\n",
            "17 [['87'], ['playoffs'], ['mike'], ['example'], ['washington'], ['91'], ['93'], ['age'], ['says'], ['just']] \n",
            "\n",
            "18 [['planetary'], ['range'], ['size'], ['volume'], ['offers'], ['bios'], ['age'], ['result'], ['centers'], ['s1']] \n",
            "\n",
            "19 [['text'], ['throw'], ['militia'], ['386'], ['flames'], ['theory'], ['35'], ['null'], ['limit'], ['away']] \n",
            "\n",
            "20 [['software'], ['look'], ['proposal'], ['brought'], ['getting'], ['quality'], ['sk'], ['lib'], ['goes'], ['saw']] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZM3TBqXvHxd",
        "outputId": "6d6ca930-8746-46b7-f013-393671d63b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "l = newsgroups_train.target_names\r\n",
        "\r\n",
        "for i in range(len(l)):\r\n",
        "  print(l[i],'\\n')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alt.atheism \n",
            "\n",
            "comp.graphics \n",
            "\n",
            "comp.os.ms-windows.misc \n",
            "\n",
            "comp.sys.ibm.pc.hardware \n",
            "\n",
            "comp.sys.mac.hardware \n",
            "\n",
            "comp.windows.x \n",
            "\n",
            "misc.forsale \n",
            "\n",
            "rec.autos \n",
            "\n",
            "rec.motorcycles \n",
            "\n",
            "rec.sport.baseball \n",
            "\n",
            "rec.sport.hockey \n",
            "\n",
            "sci.crypt \n",
            "\n",
            "sci.electronics \n",
            "\n",
            "sci.med \n",
            "\n",
            "sci.space \n",
            "\n",
            "soc.religion.christian \n",
            "\n",
            "talk.politics.guns \n",
            "\n",
            "talk.politics.mideast \n",
            "\n",
            "talk.politics.misc \n",
            "\n",
            "talk.religion.misc \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2USTKUtu8C-"
      },
      "source": [
        "Видно, что большинство тем легко угадываются по набору часто используемых слов. К примеру, 5 топик -- это явно sci.crypt (тут встречаются слова \"crypto\", \"method\", \"operation\"). Тем не менее, некоторые получившиеся наборы слов трудно различить, потому что темы близки по смыслу, а потому таковы и используемые термины. (для меня такими наборами оказались  15 и 11 -- оба связаны с оружием -- RKBA и firearms наводят мысли об этом, но какой именно топик подходит к talks.politics.guns у меня не получилось определить)"
      ]
    }
  ]
}